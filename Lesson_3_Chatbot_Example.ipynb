{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 3: Chatbot Example\n",
        "\n",
        "Build a chatbot that makes use of ArXiv for searching for papers and finding some information."
      ],
      "metadata": {
        "id": "ekLjcFG8JxrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "8MPtLZ1nJ3FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install arxiv\n",
        "! pip install python-dotenv\n",
        "! pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYJtHFnmKQRu",
        "outputId": "5b0b6a90-0747-4ea6-ea84-10367e7641e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.6.15)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=e62af96c48d7c513b574dd289aa208d727f99581d8f5827043c6e567c5943d4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.54.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.54.0-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.54.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w5Rht77cJtcB"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAPER_DIR = \"papers\" # define directory name to save papers in"
      ],
      "metadata": {
        "id": "r0K-p-4LK6gq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Papers function"
      ],
      "metadata": {
        "id": "BXWVFEOJLEgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
        "  \"\"\"\n",
        "  Search for papers on arXiv based on a topic and store their information.\n",
        "\n",
        "  Args:\n",
        "    topic: The topic to search for papers on.\n",
        "    max_results: The maximum number of results to return. (default: 5)\n",
        "\n",
        "  Returns:\n",
        "    List of paper IDs found in the search.\n",
        "  \"\"\"\n",
        "\n",
        "  # Use arxiv to find the papers\n",
        "  client = arxiv.Client()\n",
        "\n",
        "  # Search for the most relevant articles matching the queried topic\n",
        "  search = arxiv.Search(\n",
        "      query = \"all:\" + topic,\n",
        "      max_results = max_results,\n",
        "      sort_by = arxiv.SortCriterion.Relevance,\n",
        "  )\n",
        "\n",
        "  papers = client.results(search)\n",
        "\n",
        "  # Create directory for this topic\n",
        "  path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "\n",
        "  file_path = os.path.join(path, \"papers_info.json\")\n",
        "\n",
        "  # Try to load existing papers info\n",
        "  try:\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "      papers_info = json.load(json_file)\n",
        "  except (FileNotFoundError, json.JSONDecodeError):\n",
        "    papers_info = {}\n",
        "\n",
        "  # Process each paper and add to papers_info\n",
        "  paper_ids = []\n",
        "  for paper in papers:\n",
        "    paper_ids.append(paper.get_short_id())\n",
        "    paper_info = {\n",
        "        \"title\": paper.title,\n",
        "        \"summary\": paper.summary,\n",
        "        \"authors\": [author.name for author in paper.authors],\n",
        "        \"pdf_url\": paper.pdf_url,\n",
        "        \"published\": str(paper.published.date())\n",
        "    }\n",
        "    papers_info[paper.get_short_id()] = paper_info\n",
        "\n",
        "  # Save updated papers_info to json file\n",
        "  with open(file_path, \"w\") as json_file:\n",
        "    json.dump(papers_info, json_file, indent=2)\n",
        "\n",
        "  print(f\"Results are saved in: {file_path}\")\n",
        "\n",
        "  return paper_ids"
      ],
      "metadata": {
        "id": "oOY7edbHLH9A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_papers(\"computers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehkKZoplNGBZ",
        "outputId": "01ba289c-95e1-438d-f178-ed1a681ba3f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are saved in: papers/computers/papers_info.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1312.3300v1', '2207.05241v1', '2012.10468v1', '2009.00041v1', '2009.08005v1']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Information from Papers\n",
        "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
      ],
      "metadata": {
        "id": "XIjDaBTtSQxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(paper_id: str) -> str:\n",
        "  \"\"\"\n",
        "  Search for information about a specific paper across all topic directories\n",
        "\n",
        "  Args:\n",
        "    paper_id: The ID of the paper to search for.\n",
        "\n",
        "  Returns:\n",
        "    JSON string containing paper information if found, error message if not found.\n",
        "  \"\"\"\n",
        "  for item in os.listdir(PAPER_DIR):\n",
        "    item_path = os.path.join(PAPER_DIR, item)\n",
        "    if os.path.isdir(item_path):\n",
        "      file_path = os.path.join(item_path, \"papers_info.json\")\n",
        "      if os.path.isfile(file_path):\n",
        "        try:\n",
        "          with open(file_path, \"r\") as json_file:\n",
        "            papers_info = json.load(json_file)\n",
        "            if paper_id in papers_info:\n",
        "              return json.dumps(papers_info[paper_id], indent=2)\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "          print(f\"Error reading {file_path}: {str(e)}\")\n",
        "          continue\n",
        "\n",
        "  return f\"There is no saved information saved about paper {paper_id}.\""
      ],
      "metadata": {
        "id": "XzpSNitsSRoa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_info('1312.3300v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "Eukn1E8_V2Gy",
        "outputId": "48332e0c-ca48-4366-b1b9-af4b0f163013"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"title\": \"Numerical Reproducibility and Parallel Computations: Issues for Interval Algorithms\",\\n  \"summary\": \"What is called \\\\\"numerical reproducibility\\\\\" is the problem of getting the same result when the scientific computation is run several times, either on the same machine or on different machines, with different types and numbers of processing units, execution environments, computational loads etc. This problem is especially stringent for HPC numerical simulations. In what follows, the focus is on parallel implementations of interval arithmetic using floating-point arithmetic. For interval computations, numerical reproducibility is of course an issue for testing and debugging purposes. However, as long as the computed result encloses the exact and unknown result, the inclusion property, which is the main property of interval arithmetic, is satisfied and getting bit for bit identical results may not be crucial. Still, implementation issues may invalidate the inclusion property. Several ways to preserve the inclusion property are presented, on the example of the product of matrices with interval coefficients.\",\\n  \"authors\": [\\n    \"Nathalie Revol\",\\n    \"Philippe Th\\\\u00e9veny\"\\n  ],\\n  \"pdf_url\": null,\\n  \"published\": \"2013-12-11\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools List\n",
        "Each tool in the list should have at least a name and a description. We can also provide an input schema to be followed as needed.\n",
        "\n",
        "This list is kind of like YAML files, where we are simply creating a \"contract\", but not actually calling the function."
      ],
      "metadata": {
        "id": "BumhfK1BV_Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"name\": \"search_papers\",\n",
        "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"topic\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The topic to search for papers on.\",\n",
        "                },\n",
        "                \"max_results\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"The maximum number of results to return. (default: 5)\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"topic\"],\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"extract_info\",\n",
        "        \"description\": \"Search for information about a specific paper across all topic directories\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"paper_id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The ID of the paper to search for.\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"paper_id\"],\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "wPccYfXkWAu7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools Mapping\n"
      ],
      "metadata": {
        "id": "BN9BG2W7Wgnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_tool_function = {\n",
        "    \"search_papers\": search_papers,\n",
        "    \"extract_info\": extract_info\n",
        "}\n",
        "\n",
        "def execute_tool(tool_name, tool_args):\n",
        "  \"\"\"\n",
        "  Execute a tool based on its name.\n",
        "  \"\"\"\n",
        "  # Execute function to get result\n",
        "  result = mapping_tool_function[tool_name](**tool_args)\n",
        "\n",
        "  # Format result into a JSON string\n",
        "  if result is None:\n",
        "    result = \"The operation completed, but did not return any results.\"\n",
        "\n",
        "  elif isinstance(result, list):\n",
        "    # if result is a list\n",
        "    result = ', '.join(result)\n",
        "\n",
        "  elif isinstance(result, dict):\n",
        "    # if result is a dictionary\n",
        "    result = json.dumps(result, indent=2)\n",
        "\n",
        "  else:\n",
        "    # all other formats\n",
        "    result = str(result)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "e16BWESLWyek"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "77lY59ZSYASf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "vdNa0LHLcJEc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=userdata.get('ANTHROPIC_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "IikKjBF8Xyp0"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query):\n",
        "  messages = [{'role': 'user', 'content': 'query'}]\n",
        "\n",
        "  response = client.messages.create(max_tokens=2024,\n",
        "                                    model='claude-3-7-sonnet-20250219',\n",
        "                                    tools = tools,\n",
        "                                    messages = messages)\n",
        "\n",
        "  process_query = True\n",
        "\n",
        "  while process_query:\n",
        "    assistant_content = []\n",
        "\n",
        "    for content in response.content:\n",
        "      if content.type == 'text':\n",
        "        print(content.text)\n",
        "        assistant_content.append(content.text)\n",
        "\n",
        "        if len(response.content) == 1:\n",
        "          process_query = False\n",
        "\n",
        "      elif content.type == 'tool_use':\n",
        "        assistant_content.append(content)\n",
        "        messages.append({'role': 'assistant', 'content': assistant_content})\n",
        "\n",
        "        tool_id = content.id\n",
        "        tool_args = content.input\n",
        "        tool_name = content.name\n",
        "        print(f\"Calling tool {tool_name} with args: {tool_args}\")\n",
        "\n",
        "        result = execute_tool(tool_name, tool_args)\n",
        "        messages.append({'role':'user',\n",
        "                         'content': [\n",
        "                             {\"type\": \"tool_result\",\n",
        "                              \"tool_use_id\": tool_id,\n",
        "                              \"content\": result\n",
        "                              }\n",
        "                          ]\n",
        "                         })\n",
        "\n",
        "        response = client.messages.create(max_tokens=2024,\n",
        "                                          model='claude-3-7-sonnet-20250219',\n",
        "                                          tools = tools,\n",
        "                                          messages = messages)\n",
        "\n",
        "        if len(response.content) == 1 and response.content[0].type == 'text':\n",
        "          print(response.content[0].text)\n",
        "          process_query = False"
      ],
      "metadata": {
        "id": "E6WNY481YCzh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Loop"
      ],
      "metadata": {
        "id": "29MhstVYaO0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_loop():\n",
        "  print(\"Type your queries or 'quit' to exit.\")\n",
        "  while True:\n",
        "    try:\n",
        "      query = input(\"\\nQuery: \").strip()\n",
        "      if query.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "      process_query(query)\n",
        "      print(\"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"\\nAn error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "x8L5F9DGaQ6r"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfIiNzWVa03s",
        "outputId": "3693e9e6-f7d2-4651-a6eb-4687badc3534"
      },
      "execution_count": 53,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type your queries or 'quit' to exit.\n",
            "\n",
            "Query: hi\n",
            "\n",
            "An error occurred: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}\n",
            "\n",
            "Query: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concluding remarks\n",
        "- No persistent memory here. Nothing is actually being saved here, and each conversation is a brand new start."
      ],
      "metadata": {
        "id": "lpfn5PjkeCW-"
      }
    }
  ]
}